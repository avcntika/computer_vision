{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FbUF3qoBhCK"
   },
   "source": [
    "## What is object detection?\n",
    "\n",
    "<!-- > Object detection is a computer vision and image processing system that detects instances of semantic items of a certain class (such as individuals, buildings, or vehicles) in digital photos and videos. It enables the recognition and localization of objects in an image or video. Object detection methods identify objects by using specialized aspects of object classes, and there are neural network-based and non-neural ways for detecting things. Image annotation, vehicle counting, activity recognition, face detection, face recognition, and video object co-segmentation are just a few of the computer vision jobs that can benefit from this approach. It is essential in a variety of applications, including image retrieval and video surveillance. -->\n",
    "\n",
    "> Object detection consists of two tasks; namely classification and localization. Using object detection, we can not only identify what class does the object belong, but also get its position. Object detection is finding **what** and **where** (multiple) objects are in an image.\n",
    "\n",
    "> Unlike simple classification outputs bunch of probabilities, an output of an object detection model is the probability of the object belonging to a class, with four continous values that signify the position of the object.\n",
    "\n",
    "$\\hat{y} = [p, x_1, y_1, x_2, y_2]$\n",
    "\n",
    "> The position of object is said to be in a *bounding box*. The bounding box signifies the area in which the object is supposed to lie. There are two common ways to define bounding boxes:\n",
    "  - ($x_1, y_1$) is upper left corner point, ($x_2, y_2$) is bottom right corner point.\n",
    "  - Two points signify the centre point of the object and the other two values signify the height and width of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_ADBIWJBkhK"
   },
   "source": [
    "## RCNN model\n",
    "Instead of sliding-window approach, RCNN is the first region based architecture\n",
    "Main components of RCNN:\n",
    "- Region proposals: Regions are proposed in this stage. These regions indicate that there is a high chance an object lies is present here. The **selective search** algorithm scans the input image to find regions that contains blobs and proposes Regions of Interests.\n",
    "- Feature extraction: A pretrained CNN model is used on top of the region proposals to extract features from each of these candidates. The pretrained network is fine-tuned on the warped region proposals using the softmax classification output layer.\n",
    "- Classification: A Linear SVM model is used to perform classification on the features extracted using the pretrained network on the candidate region-proposals. Linear SVM perform one vs rest classification, to categorize an object into a particular class.\n",
    "\n",
    "Disadvantages of RCNN:\n",
    "1. Object detection is very slow\n",
    "1. 3-stage trianing process\n",
    "1. Two stage pipeline\n",
    "1. Training is more expensive in terms of time and space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrQes8FCBnxq"
   },
   "source": [
    "## Fast RCNN\n",
    "Fast RCNN consists of a pretrained CNN model, which has its pooling layer replaced by an **ROI Pooling** layer and the FC layer is replaced by two branches -- (K + 1) category softmax layer branch and a category specific bounding box branch.\n",
    "\n",
    "The entire image is fed into the backbone CNN model and the features from the last convolutional layer are obtained. Depending on the backbone CNN used, the output feature maps are much smaller than the original image size.\n",
    "\n",
    "The region proposal windows are obtained from a region proposal algorithm like 'selection search'.\n",
    "\n",
    "The ROIs generated by the 'selective search' algorithm are then projected onto the feature maps by multiplying the subsampling ratio. This is then passed through the ROI pooling layer. The projected ROIs on the feature maps are pooled and passed on to the fully-connected layers. A 7x7 grid is used to perform ROI pooling on a feature map which produces a vector of size 49x1. ROI pooling takes the max value from each grid.\n",
    "\n",
    "The pooling output is then fed into the successive FC layers, and the softmax and BB-regerssion branches. The softmax classification branch produces  probabiity values of each ROI belonging to K categories and one catch-all background category. The BB regression branch output is used to make the bounding boxes from the region proposal algorithm more precise.\n",
    "\n",
    "Disadvantages of Fast RCNN:\n",
    "1. Still a two stage pipeline\n",
    "1. Selective search algorithm still remains to be a bottleneck as it is very slow to generate region proposals. 2 seconds of inference time is too much for real time object detection in videos.\n",
    "\n",
    "\n",
    "#### What is ROI pooling:\n",
    "\n",
    "In object detection and recognition tasks, Region of Interest (ROI) pooling is a technique used in the Fast R-CNN algorithm to extract fixed-length feature vectors from regions of varying sizes within an input image.\n",
    "\n",
    "The Fast R-CNN algorithm first proposes candidate regions of interest in the image using a selective search algorithm or some other region proposal method. These proposed regions are then pooled into fixed-size feature maps using ROI pooling.\n",
    "\n",
    "The ROI pooling layer takes as input the proposed regions of interest and the feature maps generated by a convolutional neural network (CNN) applied to the input image. The proposed regions of interest are first transformed into a fixed size using a process called ROI warping, which resizes the proposed region of interest to a fixed spatial extent while preserving the aspect ratio.\n",
    "\n",
    "Then, the features within each of the fixed-sized regions are pooled using max-pooling, resulting in a fixed-size feature map for each proposed region of interest. These feature maps are then fed into a fully connected layer that produces a feature vector for each proposed region of interest.\n",
    "\n",
    "The ROI pooling layer is important because it allows the Fast R-CNN algorithm to handle variable-sized inputs and variable-sized regions of interest within the input image. By pooling the features within each proposed region of interest into a fixed-size feature map, the Fast R-CNN algorithm can extract useful features for object detection and recognition while maintaining spatial information about the regions of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast RCNN is a deep learning algorithm used for object detection and recognition. It starts by using a pre-trained CNN model to extract features from an input image. These features are obtained from the last convolutional layer of the CNN and are much smaller in size compared to the original image.\n",
    "\n",
    "Next, candidate regions of interest (ROIs) are generated using a region proposal algorithm like 'selective search'. The ROIs are then projected onto the feature maps by multiplying them with a subsampling ratio and passed through an ROI pooling layer. The ROI pooling layer extracts fixed-size feature maps from the variable-sized ROIs using max-pooling.\n",
    "\n",
    "The output from the ROI pooling layer is then fed into the successive fully-connected layers, followed by two branches - a softmax layer for classifying each ROI into one of K+1 categories (K object classes and a background class) and a category-specific bounding box regression layer for refining the bounding box coordinates.\n",
    "\n",
    "During training, the algorithm minimizes two losses - a classification loss and a bounding box regression loss - using backpropagation. The resulting model can then be used to detect objects in new images by proposing ROIs and passing them through the trained Fast RCNN model to obtain the class and bounding box coordinates for each detected object.\n",
    "\n",
    "In summary, Fast RCNN is an effective algorithm for object detection and recognition that uses pre-trained CNN models, ROI pooling layers, and a multi-task loss function to generate accurate predictions for object classes and bounding boxes in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gPu3lVhBrR4"
   },
   "source": [
    "## Faster RCNN\n",
    "Key points:\n",
    "- It proposes a **region proposal network**, which is a fully convolutional network that generates proposals with various scales and aspect ratios. The RPN implements the terminology of **neural network with attention** to tell the object detection where to look.\n",
    "- Rather than using **pyramids of images** or **pyramid of filters**, Faster RCNN introduces the concept of anchor boxes. An anchor box is a reference box of specific scale and aspect ratio. With multiple reference achor boxes, then multiple scales and aspect ratios exist for the single region. This can be thought of as a **pyramid of reference achor boxes**. Each region is then mapped to each reference anchor box, and thus detecting objects at different scales and aspect ratios.\n",
    "- The convolutional computations are shared across the RPN and the Fast R-CNN. This reduces the computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvPStq0jBuZZ"
   },
   "source": [
    "## IoU\n",
    "A metric used to quantify the performance of object detection algorithm, using the ratio of area of intersection of the predicted box and ground truth bounding box, to the union of the area of predicted box and ground truth bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCwM0JhYBykW"
   },
   "source": [
    "## Non-max supression\n",
    "**Non maximum suppression** is a computer vision method that selects a single entity out of many overlapping entities (for example bounding boxes in object detection). The criteria is usually discarding entities that are below a given probability bound. With remaining entities we repeatedly pick the entity with the highest probability, output that as the predicted, and discard any remaining box where $IoU \\geq 0.5$ with the box output in the previous step.\n",
    "\n",
    "- Start with discarding all bounding boxes < probability threshold\n",
    "- While BoundingBoxes:\n",
    "  - Take out the lasrgest probability box\n",
    "  - Remove all other boxes with IoU > threshold\n",
    "- Do this for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adJ6d7hrB4Wp"
   },
   "source": [
    "## YOLO: You only look once\n",
    "- The input image is divided into an $s*s$ grid. If the center of an object falls into a grid cell, that grid cell is reponsible for detecting that object.\n",
    "- Each grid cell predicts $B$ bounding boxes and confidence scores for those boxes.\n",
    "- These confidence scores reflect how confident the model is that the boc contains an object i.e. the probability of prediction of the box.\n",
    "- Each bouding box consists of 5 predictions: x, y, w, h and confidence.\n",
    "  - The $(x, y)$ coordinates represent the center of the box relative to the bounds of the grid cell.\n",
    "  - The width $w$ and height $h$ are predicted relative to the whole image.\n",
    "  - The confidence represents the Intersection Over Union (IOU) between the predicted box and any ground truth box.\n",
    "\n",
    "YOLO algorithm works using the three techniques:\n",
    "- Residual blocks: The image is divided into various grids. Each grid has a dimension of $S*S$. Every grid will detect objects that appear within them. If an object center appears within a certain grid cell, then this cell will be responsible for detecting it.\n",
    "- Bounding box regression: YOLO uses a single bounding box regression to predict the height, width, center, and class of objects.\n",
    "- IOU: The IOU is eqaul to 1 if the predicted bounding box is same as the real box. This mechanism eliminates bounding boxes that are not equal to the real box.\n",
    "- Non-Max Supression: Setting a threshold for the IOU is not always enough because an object can have multiple boxes with IOU byond the threshold, and leaving all those boxes might include noise. Using Non-max suppression, we only keep the boxes with the highest probability score of detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhTZv1w6EhGR"
   },
   "source": [
    "## Canny edge\n",
    "Canny edge detection is an advanced image processing technique used in computer vision to identify the edges of objects in an image. Canny edge detection is a fundamental technique in the field of computer vision that is primarily used for image processing. The technique's overall objective is to identify edges in an image with high accuracy and low false-positive rates. To start with, a Gaussian filter is applied to smooth the image to reduce noise. Once the image is smoothed, the technique computes the gradient magnitude and direction of the image by applying Sobel operators, which are convolutional filters. The operator computes the gradient vector for each pixel in the image, and the gradient magnitude represents the strength of the edges while the direction represents the orientation of the edges.\n",
    "\n",
    "After computing the gradient magnitude and direction, the technique applies a non-maximum suppression technique to eliminate non-essential edge points. The non-maximum suppression takes advantage of the fact that edges will have a local maximum in the gradient direction, leading to the retention of the edge pixels that have the highest magnitude.\n",
    "\n",
    "The edge detection process continues by applying double thresholding techniques. The technique involves setting two thresholds - a low threshold and a high threshold. Edges pixels whose magnitude is above the high threshold are considered strong edges, while those between the high and low thresholds are considered weak edges. Weak edges that are not part of a strong edge will be eliminated, while those that are part of strong edges will be retained.\n",
    "\n",
    "The final step in the Canny edge detection process is edge tracking by hysteresis. This step connects weak edges that are part of strong edges with a continuous line. By doing so, the Canny edge detection is able to obtain a highly accurate edge map that identifies the true edges in the image with few false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5MSWg1kEkTx"
   },
   "source": [
    "## SIFT\n",
    "SIFT (Scale-Invariant Feature Transform) is a widely used computer vision technique for image feature extraction and matching. SIFT keypoints are local features of an image that are invariant to scale, orientation, and affine distortion. SIFT algorithm works in steps as follows:\n",
    "1. Scale-space extrema detection: The first step in SIFT is to detect the key points (or keypoints) in an image that are stable across scale and orientation changes. This is done by constructing a scale space representation of the image and then by detecting extrema of the difference-of-Gaussian function.\n",
    "\n",
    "2. Keypoint localization: Once the keypoints are found, we must localize them accurately. This is done by fitting a 3D quadratic function to the scale-space extrema.\n",
    "\n",
    "3. Orientation assignment: SIFT computes keypoint orientations based on image gradient directions. A histogram of gradient directions is built around the keypoint and the peak in the histogram is assigned as the orientation of the keypoint.\n",
    "\n",
    "4. Descriptor computation: After orientation assignment, a feature descriptor is computed for each keypoint. The descriptor is a 128-dimensional vector that captures the local appearance around the keypoint. The vector is computed using a weighted histogram of gradient directions of the image patch around the keypoint.\n",
    "\n",
    "5. Matching: Once the feature descriptors are computed for keypoints in multiple images, SIFT matches keypoints between the images using a distance metric. The nearest neighbor or best match is selected based on Euclidean distance between feature descriptors.\n",
    "\n",
    "6. Outlier rejection: To remove outliers in the matching process, keypoints with a large ratio of second-best match distance to the best-match distance are rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBr5Gu6kEokF"
   },
   "source": [
    "## CNNs\n",
    "https://medium.com/@ishandandekar/introduction-to-convolutional-neural-networks-part-1-c02b9fa3bcf2\n",
    "https://medium.com/@ishandandekar/introduction-to-convolutional-neural-networks-part-2-aab33e76cea1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SHWupNI85DG"
   },
   "source": [
    "\n",
    "\n",
    "## Compare Object Detection algos\n",
    "=>\n",
    "1. Speed: YOLO (You Only Look Once) is the fastest of these algorithms because it takes a single pass over the image and predicts bounding boxes directly. R-CNN and Fast R-CNN are much slower as they run a CNN on every proposed region. Faster R-CNN is faster than R-CNN but slower than YOLO due to its use of Region Proposal Network (RPN) to generate region proposals.\n",
    "\n",
    "2. Localization accuracy: In terms of localization accuracy, Faster R-CNN generally performs better than the other three algorithms. R-CNN and Fast R-CNN are known to have lower localization accuracy compared to Faster R-CNN, while YOLO is known to have good localization accuracy but with lower precision.\n",
    "\n",
    "3. Detection accuracy: Faster R-CNN and YOLO are known to have higher accuracy in detecting objects compared to R-CNN and Fast R-CNN.\n",
    "\n",
    "4. Training time: R-CNN and Fast R-CNN take much longer to train compared to Faster R-CNN and YOLO due to their need to train multiple models.\n",
    "\n",
    "5. Object class recognition: YOLO performs better in recognizing multiple object classes in an image, while the other algorithms tend to struggle with overlapping objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HQ4lK33BZ2B"
   },
   "source": [
    "## Face recognition\n",
    "- Face identification: One-to-manu matches that compare a query face image against all the template images in the database to determine the identity of the query face\n",
    "  - Person's image is compared with all the other people iages present in the database.\n",
    "  - One to many comparison\n",
    "\n",
    "- Face verification: One-to-one match that compares a query face image against a template face image whose identity is being claimed.\n",
    "  - Person's image is saved in the database, Person's new input image is compared with the existing image in the database.\n",
    "  - One to one comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn4SQwvhNP_B"
   },
   "source": [
    "## Face verification\n",
    "- Face image embeddings of user stored in a database\n",
    "- User provides an input image, saying he/she/they are person A\n",
    "- Distance is calculated using the L2 norm of the user's face embeddings and personA's face embeddings\n",
    "  $Distance = L_2(User's face-embeddings, Person A's face-emebddings)$\n",
    "- A distance threshold is set. If the computed distance is than the distance threshold, then the person is verified as person A\n",
    "- Else: It is not person A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbH5XntJONDd"
   },
   "source": [
    "## Face identification\n",
    "- Face image embeddings of all users are stored in a database\n",
    "- Person of interest is identified\n",
    "- Person's face embeddings calculated\n",
    "- Distance is computed using the L2 norm value for person's face and face embeddings in the database\n",
    "- If distance is less than a distance threshold then the person is marked as the person in db. Else person is not in db."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhPcljXTQLjk"
   },
   "source": [
    "## Siamese network\n",
    "A **Siamese neural network** is an artificial neural network that uses the same weights while working in tandem on two different inputs to compute comparable output vectors.\n",
    "\n",
    "### Triplet loss for siamese network\n",
    "$L = max(d(a, p) - d(a, n) + margin, 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6zaqwD0lJZT"
   },
   "source": [
    "## Inception network\n",
    "Uses $1 * 1$ convolutional layer, to reduce the operational cost. Also called reduce/bottleneck layer.\n",
    "\n",
    "The idea was to add a **$1 * 1$ convolutional layer** before bigger kernels like $3*3$ and $5*5$, to reduce their depth, which in turn will reduce the number of operations.\n",
    "\n",
    "1. Inception block contains\n",
    "  - 1 x 1 convolutional layer\n",
    "  - 1 x 1 convolutional layer + 3 x 3 convolutional layer\n",
    "  - 1 x 1 convolutional layer + 5 x 5 convolutional layer\n",
    "  - 3 x 3 pooling layer + 1 x 1 convolutional layer\n",
    "1. 1 x 1 conv layers are used for depth dimensionality reduction thus reducing the floating point operations.\n",
    "1. Uses Global Average Pooling instead of Flatten\n",
    "1. Uses Auxillary classifiers for prediction and gradient propogation at intermediate parts of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrE8G9Ptmpih"
   },
   "source": [
    "## ResNet\n",
    "Downsides of deeper networks:\n",
    "1. Adding too many layers makes the network prone to overfit on the training data\n",
    "  - However ovefitting can be addressed using regularization, dropout and batch normalization\n",
    "1. Vanishing and exploding gardients\n",
    "  - During backprop, in the chained multiplication the gradient from the later layers will become very small by the time it reaches the initial layers of the network - applicable to activation functions which are diminishing in nature like `tanh` and `sigmoid`.\n",
    "  - During backprop, it also might be the case that the gradient grows exponentially quickly during the chained multiplication and takes very large values thus exploding\n",
    "\n",
    "To solve the vanishing gradient problem, ResNet uses a shortcut that allows the gradient to be directly backpropogated to earlier layers. These shortcuts are called skip connections.\n",
    "\n",
    "Skip connections allow the model to learn an indentity function which ensures that the layer will perform at least as well as the previous layer.\n",
    "\n",
    "Residual blocks:\n",
    "- Shortcut path: Connects the input to an addition of the second branch\n",
    "- Main path: A series of convolutions and activations. The main path consists of three convolutional layers with ReLU activations.\n",
    "\n",
    "Residual blocks start with **1 x 1** conv layer to downsample the input dimension volume, and a 3 x 3 conv layer and another 1 x 1 convolutional layer to downsample the output.\n",
    "This is good technique to keep control of the volume dimensions across many layers. This configuration is called a bottleneck residual block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wc2APNDmwLwd"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
